#!/usr/bin/env python3
"""
NHK ã¿ã‚“ãªã§ãƒ—ãƒ©ã‚¹ è¨˜äº‹æœ¬æ–‡ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã€æ—¥ä»˜ã€URLã€æœ¬æ–‡ã‚’å–å¾—ã—ã¦JSONå½¢å¼ã§ä¿å­˜ã—ã¾ã™
"""

import requests
from bs4 import BeautifulSoup
import json
import argparse
from typing import Dict, Optional


class NHKArticleScraper:
    def __init__(self, url: str):
        self.url = url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def scrape_article(self) -> Optional[Dict]:
        """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            response = self.session.get(self.url)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            title = self.get_title(soup)
            
            # æ—¥ä»˜ã‚’å–å¾—
            date = self.get_date(soup)
            
            # æœ¬æ–‡ã‚’å–å¾—
            content = self.get_content(soup)
            
            if not title or not content:
                print("è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
            
            return {
                'title': title,
                'date': date,
                'url': self.url,
                'content': content
            }
            
        except Exception as e:
            print(f"è¨˜äº‹ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()
            return None

    def get_title(self, soup) -> Optional[str]:
        """è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—"""
        try:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: articleå†…ã®h1ã‚¿ã‚°
            article = soup.find('article')
            if article:
                h1 = article.find('h1')
                if h1:
                    title = h1.get_text(strip=True)
                    if title:
                        return title
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: mainå†…ã®h1ã‚¿ã‚°
            main = soup.find('main')
            if main:
                h1 = main.find('h1')
                if h1:
                    title = h1.get_text(strip=True)
                    if title:
                        return title
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³3: å…¨ä½“ã‹ã‚‰h1ã‚¿ã‚°
            h1 = soup.find('h1')
            if h1:
                title = h1.get_text(strip=True)
                if title:
                    return title
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: titleã‚¿ã‚°ã‹ã‚‰å–å¾—
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text(strip=True)
                # " - NHK ã¿ã‚“ãªã§ãƒ—ãƒ©ã‚¹"ãªã©ã®ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤
                parts = title.split(' - ')
                if len(parts) > 0:
                    return parts[0].strip()
            
            return None
        except Exception as e:
            print(f"ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def get_date(self, soup) -> Optional[str]:
        """å…¬é–‹æ—¥ã‚’å–å¾—"""
        try:
            # æ—¥ä»˜ã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: time ã‚¿ã‚°
            time_tag = soup.find('time')
            if time_tag:
                return time_tag.get_text(strip=True)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ—¥ä»˜ã£ã½ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¢ã™ï¼ˆYYYYå¹´MæœˆDæ—¥å½¢å¼ï¼‰
            import re
            date_pattern = re.compile(r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥')
            
            # article ã‚¿ã‚°å†…ã‚’æ¢ã™
            article = soup.find('article')
            if article:
                date_match = date_pattern.search(article.get_text())
                if date_match:
                    return date_match.group(0)
            
            # main ã‚¿ã‚°å†…ã‚’æ¢ã™
            main = soup.find('main')
            if main:
                # mainã®æœ€åˆã®æ–¹ã«æ—¥ä»˜ãŒã‚ã‚‹ã“ã¨ãŒå¤šã„
                first_section = main.find(['div', 'section', 'header'])
                if first_section:
                    date_match = date_pattern.search(first_section.get_text())
                    if date_match:
                        return date_match.group(0)
            
            return None
        except Exception as e:
            print(f"æ—¥ä»˜ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def get_content(self, soup) -> Optional[str]:
        """è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—"""
        try:
            content_parts = []
            
            # main ã¾ãŸã¯ article ã‚¿ã‚°ã‚’æ¢ã™
            main_content = soup.find('main') or soup.find('article')
            
            if not main_content:
                print("è¨˜äº‹æœ¬æ–‡ã®ã‚³ãƒ³ãƒ†ãƒŠãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return None
            
            # è¦‹å‡ºã—ã¨æ®µè½ã‚’å–å¾—
            for element in main_content.find_all(['h2', 'h3', 'h4', 'p', 'blockquote']):
                # ä¸è¦ãªè¦ç´ ã‚’ã‚¹ã‚­ãƒƒãƒ—
                # ã‚¯ãƒ©ã‚¹åã§åˆ¤æ–­
                element_class = element.get('class', [])
                if any(cls in element_class for cls in ['share', 'sns', 'related', 'tag']):
                    continue
                
                text = element.get_text(strip=True)
                
                # ç©ºã®è¦ç´ ã‚„ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ã‚’ã‚¹ã‚­ãƒƒãƒ—
                if not text or text in ['INDEX', 'ã‚·ã‚§ã‚¢ã™ã‚‹', 'ã‚‚ã£ã¨è¦‹ã‚‹']:
                    continue
                
                # è¦‹å‡ºã—ã®å ´åˆ
                if element.name in ['h2', 'h3', 'h4']:
                    content_parts.append(f"\n## {text}\n")
                # å¼•ç”¨ã®å ´åˆ
                elif element.name == 'blockquote':
                    content_parts.append(f"\n{text}\n")
                # æ®µè½ã®å ´åˆ
                else:
                    content_parts.append(text)
            
            # çµåˆã—ã¦è¿”ã™
            content = '\n\n'.join(content_parts).strip()
            return content if content else None
            
        except Exception as e:
            print(f"æœ¬æ–‡ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return None


def main():
    parser = argparse.ArgumentParser(
        description='NHK ã¿ã‚“ãªã§ãƒ—ãƒ©ã‚¹ è¨˜äº‹æœ¬æ–‡ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼'
    )
    parser.add_argument(
        'url',
        help='è¨˜äº‹ãƒšãƒ¼ã‚¸ã®URL'
    )
    parser.add_argument(
        '-o', '--output',
        default='article.json',
        help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: article.json)'
    )
    parser.add_argument(
        '--pretty',
        action='store_true',
        help='æ•´å½¢ã•ã‚ŒãŸJSONã‚’å‡ºåŠ›'
    )
    
    args = parser.parse_args()
    
    print(f"è¨˜äº‹ã®å–å¾—ã‚’é–‹å§‹ã—ã¾ã™: {args.url}")
    print()
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’åˆæœŸåŒ–
    scraper = NHKArticleScraper(args.url)
    
    # è¨˜äº‹ã‚’å–å¾—
    article = scraper.scrape_article()
    
    if not article:
        print("è¨˜äº‹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    print("âœ… è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
    print(f"  ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}")
    print(f"  æ—¥ä»˜: {article['date']}")
    print(f"  æœ¬æ–‡: {len(article['content'])}æ–‡å­—")
    print()
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(args.output, 'w', encoding='utf-8') as f:
        if args.pretty:
            json.dump(article, f, ensure_ascii=False, indent=4)
        else:
            json.dump(article, f, ensure_ascii=False)
    
    print(f"ğŸ“„ è¨˜äº‹ã‚’ {args.output} ã«ä¿å­˜ã—ã¾ã—ãŸ")


if __name__ == '__main__':
    main()
