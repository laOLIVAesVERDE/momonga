#!/usr/bin/env python3
"""
NHK ã¿ã‚“ãªã§ãƒ—ãƒ©ã‚¹ ã‚³ãƒ¡ãƒ³ãƒˆä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
è¤‡æ•°ã®è¨˜äº‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä¸€æ‹¬ã§å–å¾—ã—ã¾ã™
"""

import json
import os
import time
from nhk_comment_scraper import NHKCommentScraper


def scrape_article_range(start_id: int, end_id: int, output_dir: str = "output", base_category: str = "0026"):
    """
    æŒ‡å®šã•ã‚ŒãŸç¯„å›²ã®è¨˜äº‹IDã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
    
    Args:
        start_id: é–‹å§‹è¨˜äº‹IDï¼ˆä¾‹: 1ï¼‰
        end_id: çµ‚äº†è¨˜äº‹IDï¼ˆä¾‹: 300ï¼‰
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        base_category: ã‚«ãƒ†ã‚´ãƒªIDï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0026ï¼‰
    """
    # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    category_output_dir = os.path.join(output_dir, base_category)
    os.makedirs(category_output_dir, exist_ok=True)
    
    success_count = 0
    skip_count = 0
    error_count = 0
    
    print(f"=" * 60)
    print(f"ã‚³ãƒ¡ãƒ³ãƒˆä¸€æ‹¬å–å¾—ã‚’é–‹å§‹ã—ã¾ã™")
    print(f"å¯¾è±¡: {base_category}_{start_id:03d} ã‹ã‚‰ {base_category}_{end_id:03d}")
    print(f"ã‚«ãƒ†ã‚´ãƒª: {base_category}")
    print(f"å‡ºåŠ›å…ˆ: {category_output_dir}/")
    print(f"=" * 60)
    print()
    
    for article_id in range(start_id, end_id + 1):
        article_num = f"{article_id:03d}"
        topic_id = f"{base_category}_{article_num}"
        
        # URLç”Ÿæˆ
        url = f"https://www.nhk.or.jp/minplus/{base_category}/comments/{topic_id}/index.html"
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
        output_file = os.path.join(category_output_dir, f"{topic_id}.json")
        
        print(f"[{article_id}/{end_id}] {topic_id} ã‚’å‡¦ç†ä¸­...")
        print(f"  URL: {url}")
        
        try:
            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’åˆæœŸåŒ–
            scraper = NHKCommentScraper(url)
            
            # ã¾ãšæœ€åˆã®ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦å­˜åœ¨ç¢ºèª
            import requests
            response = scraper.session.get(url)
            
            if response.status_code == 404:
                print(f"  âš ï¸  ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ404ï¼‰- ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                skip_count += 1
                print()
                continue
            elif response.status_code != 200:
                print(f"  âš ï¸  ã‚¨ãƒ©ãƒ¼ï¼ˆHTTP {response.status_code}ï¼‰- ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                skip_count += 1
                print()
                continue
            
            # ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
            comments = scraper.scrape_all_comments()
            
            if len(comments) == 0:
                print(f"  âš ï¸  ã‚³ãƒ¡ãƒ³ãƒˆãŒ0ä»¶ - ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                skip_count += 1
                print()
                continue
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(comments, f, ensure_ascii=False, indent=2)
            
            print(f"  âœ… æˆåŠŸ: {len(comments)}ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ")
            print(f"  ğŸ“„ ä¿å­˜å…ˆ: {output_file}")
            success_count += 1
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            error_count += 1
        
        print()
        
        # ã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·ã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã€å°‘ã—å¾…æ©Ÿ
        if article_id < end_id:
            time.sleep(1)  # 1ç§’å¾…æ©Ÿ
    
    # çµæœã‚µãƒãƒªãƒ¼
    print()
    print("=" * 60)
    print("ä¸€æ‹¬å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸ")
    print("=" * 60)
    print(f"ã‚«ãƒ†ã‚´ãƒª: {base_category}")
    print(f"âœ… æˆåŠŸ: {success_count}ä»¶")
    print(f"âš ï¸  ã‚¹ã‚­ãƒƒãƒ—: {skip_count}ä»¶")
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
    print(f"ğŸ“ å‡ºåŠ›å…ˆ: {category_output_dir}/")
    print("=" * 60)


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(
        description='NHK ã¿ã‚“ãªã§ãƒ—ãƒ©ã‚¹ ã‚³ãƒ¡ãƒ³ãƒˆä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼'
    )
    parser.add_argument(
        '--start',
        type=int,
        default=1,
        help='é–‹å§‹è¨˜äº‹IDï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰'
    )
    parser.add_argument(
        '--end',
        type=int,
        default=300,
        help='çµ‚äº†è¨˜äº‹IDï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 300ï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        default='output',
        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: outputï¼‰'
    )
    parser.add_argument(
        '--category',
        default='0026',
        help='ã‚«ãƒ†ã‚´ãƒªIDï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0026ï¼‰'
    )
    
    args = parser.parse_args()
    
    scrape_article_range(
        start_id=args.start,
        end_id=args.end,
        output_dir=args.output_dir,
        base_category=args.category
    )
